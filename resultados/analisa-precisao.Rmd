---
title: "Análise da precisão"
output:
  html_document:
    df_print: paged
---

```{r}
library(tidyverse)
library(here)
library(modelr)
library(broom)

theme_set(theme_bw())
```

## Carrega os dados

```{r carrega}
reclamacoes_raw = read_csv(here("data/reclamacoes-raw/reclamacoes-raw.csv"))
avaliacoes_raw = read_csv(here("data/avaliacoes/avaliacoes-20180222.csv"))
sentimentos = read_csv(here("data/sentimentos/sentimento.csv"))
sentimentos_ponderados = read_csv(here("data/sentimentos/sentimento_ponderado.csv"))

# adicionando id a reclamacoes, comprimento da reclamacao e nome do orgao
reclamacoes_raw = reclamacoes_raw %>% 
    mutate(id = 1:n(), 
           comprimento_reclamacao = str_length(reclamacao), 
           nome_orgao = str_split(link, "/") %>% map_chr(~ .[[5]]))
```

### Levando em conta o comprimento das reclamações

O autor do blogpost que estamos nos baseando comentou a respeito de possíveis *outliers* que seriam consequência do tamanho das reclamações. Assim fizemos um teste considerando o tamanho das reclamações mas iremos deixar para averiguar isto mais a fundo no futuro.

```{r levando_comprimento_em_conta}
sentimentos = sentimentos %>%
             mutate(s_op30 = sentimento_op30/palavras_op30,
                    s_sent = sentimento_sent/palavras_sent)

sentimentos_ponderados = sentimentos_ponderados %>%
             mutate(s_op30 = sentimento_op30/palavras_op30,
                    s_sent = sentimento_sent/palavras_sent)
```

`reclamacoes_l` tem um formato long em vez de wide (explicado [aqui](https://sejdemyr.github.io/r-tutorials/basics/wide-and-long/)).

```{r junta}
avaliacoes = avaliacoes_raw %>% 
    group_by(id_reclamação) %>% 
    summarise(insatisfação = median(insatisfação), 
              avaliadores = n())

reclamacoes = reclamacoes_raw %>% 
    inner_join(avaliacoes, by = c("id" = "id_reclamação")) %>% 
    left_join(sentimentos, by = "id")  %>%
    left_join(sentimentos_ponderados %>% select(sentimento_ponderado_op30 = sentimento_op30, sentimento_ponderado_sent = sentimento_sent, id), by = "id")

reclamacoes_l = reclamacoes %>%  
    select(-palavras_op30, -palavras_sent) %>% 
    gather(key = "lexico", 
           value = "polaridade", 
           sentimento_op30, sentimento_sent)

reclamacoes_l_p = reclamacoes %>%  
    select(-palavras_op30, -palavras_sent) %>% 
    gather(key = "lexico", 
           value = "polaridade", 
           sentimento_ponderado_op30, sentimento_ponderado_sent)
```

## Converte polaridades para escala 0-5

Antes de converter é importante saber alguns valores chaves para cada léxico, como o mínimo, máximo e os quartis.

```{r}
summary(reclamacoes)

min_op30 = min(reclamacoes$sentimento_op30)
max_op30 = max(reclamacoes$sentimento_op30)
first_quantile_op30 = quantile(reclamacoes$sentimento_op30, 0.25, na.rm=T)
third_quantile_op30 = quantile(reclamacoes$sentimento_op30, 0.75, na.rm=T)

min_sent = min(reclamacoes$sentimento_sent)
max_sent = max(reclamacoes$sentimento_sent)
first_quantile_sent = quantile(reclamacoes$sentimento_sent, 0.25, na.rm=T)
third_quantile_sent = quantile(reclamacoes$sentimento_sent, 0.75, na.rm=T)
```

### Normalizando da maneira usual entre 0 e 5.

Agora Iremos calcular a polaridade_normalizada_usal usando a estrategia mais simples, simplesmente normalizando os valores entre 0 e 5.

```{r}
 polarizeResultUsual <- function(Min, Max){
     range = Max - Min
     arr = (reclamacoes_l$polaridade - Min)/range
     normalized = 5 - ((arr*5) -0)  # invertendo 0-> 5 e 5->0
     return(normalized)
 }

# reclamacoes long
reclamacoes_l$polaridade_normalizada_usual <-        ifelse(reclamacoes_l$lexico=="sentimento_op30", polarizeResultUsual(min_op30, max_op30), polarizeResultUsual(min_sent, max_sent))

# reclamacoes op30
polarizeResultUsual2 <- function(Min, Max){
     range = Max - Min
     arr = (reclamacoes$sentimento_op30 - Min)/range
     normalized = 5 - ((arr*5) -0)  # invertendo 0-> 5 e 5->0
     return(normalized)
 }
reclamacoes$sentimento_op30_usual <- polarizeResultUsual2(min_op30, max_op30)

# reclamacoes sent
polarizeResultUsual3 <- function(Min, Max){
     range = Max - Min
     arr = (reclamacoes$sentimento_sent - Min)/range
     normalized = 5 - ((arr*5) -0)  # invertendo 0-> 5 e 5->0
     return(normalized)
 }
reclamacoes$sentimento_sent_usual <- polarizeResultUsual3(min_sent, max_sent)
```

### Normalizando utilizando os quartis

Como os resultados dos léxicos possuem muitos valores extremos, criaremos a polaridade_normalizada_quartil que ao invés de considerar os mínimos e máximos, considera os primeiros e terceiros quartis.

```{r}
polarizeResultQuartil <- function(Min, Max){
     range = Max - Min
     arr = (reclamacoes_l$polaridade - Min)/range
     normalized = 5 - ((arr*5) -0)
     normalized <- ifelse(arr<0, 5, normalized)
     normalized <- ifelse(arr>1, 0, normalized)
     return(normalized)
 }

# reclamacoes long
reclamacoes_l$polaridade_normalizada_quartil <-        ifelse(reclamacoes_l$lexico=="sentimento_op30", polarizeResultQuartil(first_quantile_op30, third_quantile_op30), polarizeResultQuartil(first_quantile_sent, third_quantile_sent))

# reclamacoes op30
polarizeResultQuartil2 <- function(Min, Max){
     range = Max - Min
     arr = (reclamacoes$sentimento_op30 - Min)/range
     normalized = 5 - ((arr*5) -0)
     normalized <- ifelse(arr<0, 5, normalized)
     normalized <- ifelse(arr>1, 0, normalized)
     return(normalized)
 }

reclamacoes$sentimento_op30_quartil <- polarizeResultQuartil2(first_quantile_op30, third_quantile_op30)

# reclamacoes sent
polarizeResultQuartil3 <- function(Min, Max){
     range = Max - Min
     arr = (reclamacoes$sentimento_sent - Min)/range
     normalized = 5 - ((arr*5) -0)
     normalized <- ifelse(arr<0, 5, normalized)
     normalized <- ifelse(arr>1, 0, normalized)
     return(normalized)
 }

reclamacoes$sentimento_sent_quartil <- polarizeResultQuartil3(first_quantile_sent, third_quantile_sent)
```

## Calculando o erro por reclamação

```{r}
reclamacoes_l = reclamacoes_l %>%
    mutate(erro_usual = (insatisfação - polaridade_normalizada_usual)**2,
           erro_quartil = (insatisfação - polaridade_normalizada_quartil)**2)
```


## EDA

Inicial. Faça os gráficos a mais que achar necessário para entender os dados que temos de resultado.

### Comparando os valores dos léxicos

```{r}
reclamacoes %>% 
    ggplot(aes(x = sentimento_op30, y = sentimento_sent)) + 
    geom_abline(slope = 1, intercept = 0, color = "grey") + 
    geom_count(alpha = .7) 
```

### Comparando os valores dos léxicos normalizados

#### Normalização usual

```{r}
reclamacoes %>% 
    ggplot(aes(x = sentimento_op30_usual, y = sentimento_sent_usual)) + 
    geom_abline(slope = 1, intercept = 0, color = "grey") + 
    geom_count(alpha = .7) 
```

### Normalização utilizando os quartis

```{r}
reclamacoes %>% 
    ggplot(aes(x = sentimento_op30_quartil, y = sentimento_sent_quartil)) + 
    geom_abline(slope = 1, intercept = 0, color = "grey") + 
    geom_count(alpha = .7) 
```

### Erro e comparação das normalizações

### Usual

```{r}
reclamacoes_l %>% 
    ggplot(aes(x = insatisfação, y = polaridade_normalizada_usual, group = insatisfação)) + 
    geom_jitter(alpha = .7)  + 
    facet_wrap(~ lexico)

reclamacoes_l %>% 
    ggplot(aes(x = insatisfação, y = erro_usual, group = insatisfação)) + 
    geom_jitter(alpha = .5)  +
    # geom_boxplo() + 
    facet_wrap(~ lexico)
```

### Quartil

```{r}
reclamacoes_l %>% 
    ggplot(aes(x = insatisfação, y = polaridade_normalizada_quartil, group = insatisfação)) + 
    geom_jitter(alpha = .7)  + 
    facet_wrap(~ lexico)

reclamacoes_l %>% 
    ggplot(aes(x = insatisfação, y = erro_quartil, group = insatisfação)) + 
    geom_jitter(alpha = .5)  +
    # geom_boxplo() + 
    facet_wrap(~ lexico)
```


## Há relação entre o léxico e a precisão/erro?

Agora um modelo para responder sua pergunta.

```{r}
model_erro_lex = lm(erro_usual ~ factor(lexico), data=reclamacoes_l)
tidy(model_erro_lex, conf.int = TRUE, conf.level = 0.95)
```
```{r}
glance(model_erro_lex)
```

Pelos resultados do p.value, cujo alpha está bem acima de 0.05, e do intervalo de confiança, que inclue 0, não parece haver relação entre o léxico e o erro. 

```{r}
reclamacoes_l = reclamacoes_l %>% # Adicionando predições do modelo ao dataframe
    add_predictions(model_erro_lex)
```

```{r}
reclamacoes_l %>%
    ggplot(aes(x = lexico)) +
    geom_line(aes(y = pred), colour = "red") +
    geom_point(aes(y = erro_usual)) +
    labs(y = "Erro da predição") 
    
```

# TODO: fazer regressão levando em conta o léxico e a normalização

Regressão múltipla foi utilizada para analisar se o léxico e a forma de normalização tem uma associação significativa com o erro na estimativa de instatisfação da reclamação. Os resultados da regressão indicam que um modelo com os 2 preditores no formato Erro = XXX.VarIndep1 + YYY.VarIndep2 explicam XX,XX% da variância da variável de resposta (R2 = XX,XX). VarIndep1, medida como/em [unidade ou o que é o 0 e o que é 1] tem uma relação significativa com o erro (b = [yy,yy;  zz,zz], IC com 95%), assim como VarIndep2 medida como [unidade ou o que é o 0 e o que é 1] (b = [yy,yy;  zz,zz], IC com 95%). O aumento de 1 unidade de VarIndep1 produz uma mudança de...

